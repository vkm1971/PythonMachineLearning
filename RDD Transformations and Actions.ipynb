{
    "cells": [
        {
            "cell_type": "markdown", 
            "source": "# RDD Transformations and Actions\n\nIn this lecture we will begin to delve deeper into using Spark and Python. Please view the video lecture for a full explanation.\n\n## Important Terms\n\nLet's quickly go over some important terms:\n\nTerm                   |Definition\n----                   |-------\nRDD                    |Resilient Distributed Dataset\nTransformation         |Spark operation that produces an RDD\nAction                 |Spark operation that produces a local object\nSpark Job              |Sequence of transformations on data with a final action", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Creating an RDD\n\nThere are two common ways to create an RDD:\n\nMethod                      |Result\n----------                               |-------\n`sc.parallelize(array)`                  |Create RDD of elements of array (or list)\n`sc.textFile(path/to/file)`                      |Create RDD of lines from file", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "## RDD Transformations\n\nWe can use transformations to create a set of instructions we want to preform on the RDD (before we call an action and actually execute them).\n\nTransformation Example                          |Result\n----------                               |-------\n`filter(lambda x: x % 2 == 0)`           |Discard non-even elements\n`map(lambda x: x * 2)`                   |Multiply each RDD element by `2`\n`map(lambda x: x.split())`               |Split each string into words\n`flatMap(lambda x: x.split())`           |Split each string into words and flatten sequence\n`sample(withReplacement=True,0.25)`      |Create sample of 25% of elements with replacement\n`union(rdd)`                             |Append `rdd` to existing RDD\n`distinct()`                             |Remove duplicates in RDD\n`sortBy(lambda x: x, ascending=False)`   |Sort elements in descending order", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "## RDD Actions\n\nOnce you have your 'recipe' of transformations ready, what you will do next is execute them by calling an action. Here are some common actions:\n\nAction                             |Result\n----------                             |-------\n`collect()`                            |Convert RDD to in-memory list \n`take(3)`                              |First 3 elements of RDD \n`top(3)`                               |Top 3 elements of RDD\n`takeSample(withReplacement=True,3)`   |Create sample of 3 elements with replacement\n`sum()`                                |Find element sum (assumes numeric elements)\n`mean()`                               |Find element mean (assumes numeric elements)\n`stdev()`                              |Find element deviation (assumes numeric elements)", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "____\n# Examples\n\nNow the best way to show all of this is by going through examples! We'll first review a bit by creating and working with a simple text file, then we will move on to more realistic data, such as customers and sales data.\n\n### Creating an RDD from a text file:\n\n** Creating the textfile **", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "%%writefile example2.txt\nfirst \nsecond line\nthe third line\nthen a fourth line", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "Now let's perform some transformations and actions on this text file:", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "#from pyspark import SparkContext", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "code", 
            "source": "#sc = SparkContext()\nsc", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "code", 
            "source": "# Show RDD\nsc.textFile('example2.txt')", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "code", 
            "source": "# Save a reference to this RDD\ntext_rdd = sc.textFile('example2.txt')", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "code", 
            "source": "text_rdd.collect()", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "code", 
            "source": "# Map a function (or lambda expression) to each line\n# Then collect the results.\ntext_rdd.map(lambda line: line.split()).collect()", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Map vs flatMap", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "# Collect everything as a single flat map\ntext_rdd.flatMap(lambda line: line.split()).collect()", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "# RDDs and Key Value Pairs\n\nNow that we've worked with RDDs and how to aggregate values with them, we can begin to look into working with Key Value Pairs. In order to do this, let's create some fake data as a new text file.\n\nThis data represents some services sold to customers for some SAAS business.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "%%writefile services.txt\n#EventId    Timestamp    Customer   State    ServiceID    Amount\n201       10/13/2017      100       NY       131          100.00\n204       10/18/2017      700       TX       129          450.00\n202       10/15/2017      203       CA       121          200.00\n206       10/19/2017      202       CA       131          500.00\n203       10/17/2017      101       NY       173          750.00\n205       10/19/2017      202       TX       121          200.00", 
            "execution_count": 1, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Overwriting services.txt\n", 
                    "name": "stdout"
                }
            ], 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "code", 
            "source": "services = sc.textFile('services.txt')", 
            "execution_count": 2, 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "code", 
            "source": "services.take(2)", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "code", 
            "source": "services.map(lambda x: x.split())", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "code", 
            "source": "services.map(lambda x: x.split()).take(3)", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "Let's remove that first hash-tag!", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "services.map(lambda x: x[1:] if x[0]=='#' else x).collect()", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "code", 
            "source": "services.map(lambda x: x[1:] if x[0]=='#' else x).map(lambda x: x.split()).collect()", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Using Key Value Pairs for Operations\n\nLet us now begin to use methods that combine lambda expressions that use a ByKey argument. These ByKey methods will assume that one of the \n\n\nFor example let's find out the total sales per state: ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "# From Previous\ncleanServ = services.map(lambda x: x[1:] if x[0]=='#' else x).map(lambda x: x.split())", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "code", 
            "source": "# Let's start by practicing grabbing fields\ncleanServ.map(lambda lst: (lst[3],lst[-1])).collect()", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "code", 
            "source": "# Continue with reduceByKey\n# Notice how it assumes that the first item is the key!\ncleanServ.map(lambda lst: (lst[3],lst[-1]))\\\n         .reduceByKey(lambda amt1,amt2 : amt1+amt2)\\\n         .collect()", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "Uh oh! Looks like we forgot that the amounts are still strings! Let's fix that:", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "# Continue with reduceByKey\n# Notice how it assumes that the first item is the key!\ncleanServ.map(lambda lst: (lst[3],lst[-1]))\\\n         .reduceByKey(lambda amt1,amt2 : float(amt1)+float(amt2))\\\n         .collect()", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "We can continue our analysis by sorting this output:", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "# Grab state and amounts\n# Add them\n# Get rid of ('State','Amount')\n# Sort them by the amount value\ncleanServ.map(lambda lst: (lst[3],lst[-1]))\\\n.reduceByKey(lambda amt1,amt2 : float(amt1)+float(amt2))\\\n.filter(lambda x: not x[0]=='State')\\\n.sortBy(lambda stateAmount: stateAmount[1], ascending=False)\\\n.collect()", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "** Remember to try to use unpacking for readability. For example: **", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "x = ['ID','State','Amount']", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "code", 
            "source": "def func1(lst):\n    return lst[-1]", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "code", 
            "source": "def func2(id_st_amt):\n    # Unpack Values\n    (Id,st,amt) = id_st_amt\n    return amt", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "code", 
            "source": "func1(x)", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "code", 
            "source": "func2(x)", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "# Great Job!", 
            "metadata": {}
        }
    ], 
    "nbformat_minor": 0, 
    "nbformat": 4, 
    "metadata": {
        "kernelspec": {
            "language": "python", 
            "display_name": "Python 3.5 (Experimental) with Spark 1.6", 
            "name": "python3"
        }, 
        "language_info": {
            "pygments_lexer": "ipython3", 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "name": "python", 
            "file_extension": ".py", 
            "version": "3.5.2", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }
}