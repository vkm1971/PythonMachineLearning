{
    "nbformat": 4, 
    "nbformat_minor": 0, 
    "metadata": {
        "language_info": {
            "version": "3.5.2", 
            "pygments_lexer": "ipython3", 
            "mimetype": "text/x-python", 
            "codemirror_mode": {
                "name": "ipython", 
                "version": 3
            }, 
            "name": "python", 
            "nbconvert_exporter": "python", 
            "file_extension": ".py"
        }, 
        "kernelspec": {
            "language": "python", 
            "name": "python3", 
            "display_name": "Python 3.5 (Experimental) with Spark 1.6"
        }
    }, 
    "cells": [
        {
            "source": "# Introduction to Spark and Python\n\nLet's learn how to use Spark with Python by using the pyspark library! Make sure to view the video lecture explaining Spark and RDDs before continuing on with this code.\n\nThis notebook will serve as reference code for the Big Data section of the course involving Amazon Web Services. The video will provide fuller explanations for what the code is doing.\n\n## Creating a SparkContext\n\nFirst we need to create a SparkContext. We will import this from pyspark:", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "# from pyspark import SparkContext", 
            "execution_count": 1, 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code"
        }, 
        {
            "source": "Now create the SparkContext,A SparkContext represents the connection to a Spark cluster, and can be used to create an RDD and broadcast variables on that cluster.\n\n*Note! You can only have one SparkContext at a time the way we are running things here.*", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "# sc = SparkContext()", 
            "execution_count": 4, 
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code"
        }, 
        {
            "source": "# Spark context alreday exists!\nsc", 
            "execution_count": 3, 
            "outputs": [
                {
                    "execution_count": 3, 
                    "data": {
                        "text/plain": "<pyspark.context.SparkContext at 0x7fbdf9292a90>"
                    }, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code"
        }, 
        {
            "source": "## Basic Operations\n\nWe're going to start with a 'hello world' example, which is just reading a text file. First let's create a text file.\n___", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "Let's write an example text file to read, we'll use some special jupyter notebook commands for this, but feel free to use any .txt file:", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown"
        }, 
        {
            "source": "%%writefile example.txt\nfirst line\nsecond line\nthird line\nfourth line", 
            "execution_count": 16, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Overwriting example.txt\n"
                }
            ], 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code"
        }, 
        {
            "source": "### Creating the RDD", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "Now we can take in the textfile using the **textFile** method off of the SparkContext we created. This method will read a text file from HDFS, a local file system (available on all\nnodes), or any Hadoop-supported file system URI, and return it as an RDD of Strings.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "textFile = sc.textFile('example.txt')", 
            "execution_count": 17, 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code"
        }, 
        {
            "source": "Spark\u2019s primary abstraction is a distributed collection of items called a Resilient Distributed Dataset (RDD). RDDs can be created from Hadoop InputFormats (such as HDFS files) or by transforming other RDDs. \n\n### Actions\n\nWe have just created an RDD using the textFile method and can perform operations on this object, such as counting the rows.\n\nRDDs have actions, which return values, and transformations, which return pointers to new RDDs. Let\u2019s start with a few actions:", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "textFile.count()", 
            "execution_count": 18, 
            "outputs": [
                {
                    "execution_count": 18, 
                    "data": {
                        "text/plain": "4"
                    }, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code"
        }, 
        {
            "source": "textFile.first()", 
            "execution_count": 19, 
            "outputs": [
                {
                    "execution_count": 19, 
                    "data": {
                        "text/plain": "'first line'"
                    }, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code"
        }, 
        {
            "source": "### Transformations\n\nNow we can use transformations, for example the filter transformation will return a new RDD with a subset of items in the file. Let's create a sample transformation using the filter() method. This method (just like Python's own filter function) will only return elements that satisfy the condition. Let's try looking for lines that contain the word 'second'. In which case, there should only be one line that has that.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "secfind = textFile.filter(lambda line: 'second' in line)", 
            "execution_count": 20, 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code"
        }, 
        {
            "source": "# RDD\nsecfind", 
            "execution_count": 21, 
            "outputs": [
                {
                    "execution_count": 21, 
                    "data": {
                        "text/plain": "PythonRDD[12] at RDD at PythonRDD.scala:43"
                    }, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code"
        }, 
        {
            "source": "# Perform action on transformation\nsecfind.collect()", 
            "execution_count": 22, 
            "outputs": [
                {
                    "execution_count": 22, 
                    "data": {
                        "text/plain": "['second line']"
                    }, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code"
        }, 
        {
            "source": "# Perform action on transformation\nsecfind.count()", 
            "execution_count": 23, 
            "outputs": [
                {
                    "execution_count": 23, 
                    "data": {
                        "text/plain": "1"
                    }, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code"
        }, 
        {
            "source": "Notice how the transformations won't display an output and won't be run until an action is called. In the next lecture: Advanced Spark and Python we will begin to see many more examples of this transformation and action relationship!\n\n# Great Job!", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code"
        }
    ]
}